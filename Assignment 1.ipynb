{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88afbcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e190fed6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb46a08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "876aad94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request failed with status code: 503\n",
      "Retryin\n",
      "Request failed with status code: 503\n",
      "Retryin\n",
      "Request failed with status code: 503\n",
      "Retryin\n",
      "Request failed with status code: 503\n",
      "Retryin\n",
      "Request failed with status code: 503\n",
      "Retryin\n",
      "Null\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "\n",
    "url = \"https://www.amazon.in/s?k=bags&crid=2M096C61O4MLT&qid=1653308124&sprefix=ba%2Caps%2C283&ref=sr_pg_1\"\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36',\n",
    "    'Accept-Language': 'en-US, en;q=0.5'\n",
    "}\n",
    "\n",
    "def scrape_website(url, headers):\n",
    "    max_retries = 5\n",
    "    retry_delay = 2\n",
    "    \n",
    "    for i in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers)\n",
    "            if response.status_code == 200:\n",
    "                print(\"OK\")\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "               \n",
    "                return\n",
    "            else:\n",
    "                print(f\"Request failed with status code: {response.status_code}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error: {e}\")\n",
    "        \n",
    "        print(\"Retryin\")\n",
    "        time.sleep(retry_delay)\n",
    "    \n",
    "    print(\"Null\")\n",
    "\n",
    "scrape_website(url, headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88cf228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASSIGNMENT 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ec7962dd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Product URL  \\\n",
      "0    https://www.amazon.in/POLESTAR-Vintage-Laptop-...   \n",
      "1    https://www.amazon.in/ADISA-BP005-Weight-Casua...   \n",
      "2    https://www.amazon.in/Safari-Laptop-Backpack-R...   \n",
      "3    https://www.amazon.in/Safari-Laptop-Backpack-C...   \n",
      "4    https://www.amazon.in/gp/bestsellers/luggage/2...   \n",
      "..                                                 ...   \n",
      "157  https://www.amazon.in/Wildcraft-Ltrs-orange-Ru...   \n",
      "158  https://www.amazon.in/Puma-Unisex-Adult-Backpa...   \n",
      "159  https://www.amazon.in/PUMA-Laptop-Backpack-CAS...   \n",
      "160  https://www.amazon.in/Skybags-Unisex-Black-Fab...   \n",
      "161  https://www.amazon.in/Genie-Clara-Backpack-Wom...   \n",
      "\n",
      "                                          Product Name  Product Price  Rating  \\\n",
      "0    POLESTAR Vintage 15.6 inch 32 LTR Casual Lapto...          549.0     3.9   \n",
      "1           ADISA Light Weight Laptop Backpack 32 Ltrs          499.0     4.0   \n",
      "2    Safari Quill Laptop Backpack 26 Ltrs Water Res...          899.0     4.0   \n",
      "3    Safari Tribe 35 Ltrs Large Laptop Backpack wit...         1099.0     3.8   \n",
      "4    Sfane Polyester 23cms Duffle/Shoulder/Gym Bag ...          387.0     4.3   \n",
      "..                                                 ...            ...     ...   \n",
      "157         Wildcraft 45 Ltrs Grey and Orange Rucksack         1739.0     4.3   \n",
      "158  Puma Unisex-Adult Core Backpack, Parisian Nigh...          911.0     5.0   \n",
      "159                           PUMA Laptop Backpack IND         1059.0     3.9   \n",
      "160    Skybags Unisex Black Gucci Fabric 10L Backpacks          392.0     3.9   \n",
      "161  Genie Clara Backpack for Women - 3 compartment...         1279.0     4.4   \n",
      "\n",
      "     Number of Reviews  \n",
      "0               8966.0  \n",
      "1               5968.0  \n",
      "2                682.0  \n",
      "3               1577.0  \n",
      "4               7480.0  \n",
      "..                 ...  \n",
      "157             9013.0  \n",
      "158                1.0  \n",
      "159               39.0  \n",
      "160              103.0  \n",
      "161              183.0  \n",
      "\n",
      "[162 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def scrape_product_details(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36',\n",
    "        'Accept-Language': 'en-US, en;q=0.5'\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    product_urls = []\n",
    "    product_names = []\n",
    "    product_prices = []\n",
    "    product_ratings = []\n",
    "    product_reviews = []\n",
    "    product_asin = []\n",
    "\n",
    "    #product_list = soup.find_all('div', {'class': 's-card-container s-overflow-hidden aok-relative puis-wide-grid-style puis-wide-grid-style-t1 puis-include-content-margin puis puis-'}) # tried a couple of times but I received Null Data\n",
    "    product_list = soup.find_all('div', {'data-component-type': 's-search-result'}) # took help from another source\n",
    "    for product in product_list:\n",
    "        url_elem = product.find('a', {'class': 'a-link-normal'})\n",
    "        url = 'https://www.amazon.in' + url_elem['href'] if url_elem else None\n",
    "        product_urls.append(url)\n",
    "        \n",
    "        name_elem = product.find('span', {'class': 'a-size-medium a-color-base a-text-normal'})\n",
    "        name = name_elem.text.strip() if name_elem else None\n",
    "        product_names.append(name)\n",
    "        \n",
    "        price_elem = product.find('span', {'class': 'a-price-whole'})\n",
    "        price = price_elem.text.replace(',', '') if price_elem else None\n",
    "        price = price.replace('M.R.P: ', '') if price else None\n",
    "        product_prices.append(float(price) if price else None)\n",
    "        \n",
    "        rating_elem = product.find('span', {'class': 'a-icon-alt'})\n",
    "        rating = rating_elem.text.split()[0] if rating_elem else None\n",
    "        product_ratings.append(float(rating) if rating else None)\n",
    "        \n",
    "        reviews_elem = product.find('span', {'class': 'a-size-base'})\n",
    "        reviews = reviews_elem.text.replace(',', '') if reviews_elem else None\n",
    "        reviews = re.sub(r'\\D', '', reviews) if reviews else None\n",
    "        product_reviews.append(int(reviews) if reviews else None)\n",
    "        \n",
    "\n",
    "    data = {\n",
    "        'Product URL': product_urls,\n",
    "        'Product Name': product_names,\n",
    "        'Product Price': product_prices,\n",
    "        'Rating': product_ratings,\n",
    "        'Number of Reviews': product_reviews,\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "def scrape_multiple_pages(base_url, num_pages):\n",
    "    all_data = []\n",
    "    for page in range(1, num_pages + 1):\n",
    "        url = base_url + f'&page={page}'\n",
    "        data = scrape_product_details(url)\n",
    "        all_data.append(data)\n",
    "    \n",
    "    result = pd.concat(all_data, ignore_index=True)\n",
    "    return result\n",
    "\n",
    "base_url = 'https://www.amazon.in/s?k=bags&crid=2M096C61O4MLT&qid=1653308124&sprefix=ba%2Caps%2C283&ref=sr_pg_1'\n",
    "num_pages = 30\n",
    "\n",
    "result = scrape_multiple_pages(base_url, num_pages)\n",
    "print(result)\n",
    "result.to_csv('Assignment 1.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d750ed99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1a0d8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e7471d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
